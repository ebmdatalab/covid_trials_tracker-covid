{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ICTRP Search: \"covid-19\" or \"novel coronavirus\" or \"2019-ncov\" or \"covid19\" or \"sars-cov-2\"\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "#POINT THIS TO THE UPDATED XML - deprecated as ICTRP is down.\n",
    "\n",
    "#with open('ICTRP-Results_18Mar2020.xml', 'rb') as f:\n",
    "#    xml = xmltodict.parse(f, dict_constructor=dict)\n",
    "\n",
    "#df = pd.DataFrame(xml['Trials_downloaded_from_ICTRP']['Trial'])\n",
    "\n",
    "#This now takes the CSV posted by the ICTRP as an input from here: https://www.who.int/ictrp/en/\n",
    "\n",
    "df = pd.read_excel('this_weeks_data/COVID19-26may2020.xlsx', dtype={'Phase': str})\n",
    "\n",
    "#UPDATE THESE WITH EACH RUN\n",
    "prior_extract_date = date(2020,5,19)\n",
    "this_extract_date = date(2020,5,26)\n",
    "\n",
    "def fix_dates(x):\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            return x\n",
    "        else:\n",
    "            return x.date()\n",
    "    except AttributeError:\n",
    "        return x\n",
    "    \n",
    "def d_c(x):\n",
    "    return x[x.TrialID.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date enrollement'] = pd.to_datetime(df['Date enrollement'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting target enrollment\n",
    "size = df['Target size'].tolist()\n",
    "\n",
    "extracted_size = []\n",
    "for s in size:\n",
    "    if not s or isinstance(s,float):\n",
    "        extracted_size.append('Not Available')\n",
    "    elif isinstance(s,int):\n",
    "        extracted_size.append(s)\n",
    "    elif isinstance(s,str):\n",
    "        digits = []\n",
    "        nums = re.findall(r':\\d{1,10};',s)\n",
    "        for n in nums:\n",
    "            digits.append(int(n.replace(':','').replace(';','')))\n",
    "        extracted_size.append(sum(digits))\n",
    "    else:\n",
    "        print(type(s))\n",
    "\n",
    "df['target_enrollment'] = extracted_size\n",
    "\n",
    "#Creating retrospective registration\n",
    "df['retrospective_registration'] = np.where(df['Date registration'] > df['Date enrollement'], True, False)\n",
    "df['Date enrollement'] = df['Date enrollement'].fillna('No Date Available').apply(fix_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ICTRP shows 2936 trials as of 2020-05-26\n"
     ]
    }
   ],
   "source": [
    "#Taking only what we need right now\n",
    "\n",
    "cols = ['TrialID', 'Source Register', 'Date registration', 'Date enrollement', 'retrospective_registration', \n",
    "        'Primary sponsor', 'Recruitment Status', 'Phase', 'Study type', 'Countries', 'Public title', \n",
    "        'Intervention', 'target_enrollment', 'web address', 'results yes no', 'results url link', \n",
    "        'Last Refreshed on']\n",
    "\n",
    "df_cond = df[cols].reset_index(drop=True)\n",
    "\n",
    "#renaming columns to match old format so I don't have to change them throughout\n",
    "df_cond.columns = ['TrialID', 'Source_Register', 'Date_registration', 'Date_enrollement', \n",
    "                   'retrospective_registration', 'Primary_sponsor', 'Recruitment_Status', 'Phase', 'Study_type', \n",
    "                   'Countries', 'Public_title', 'Intervention', 'target_enrollment', 'web_address', \n",
    "                   'has_results', 'results_url_link', 'Last_Refreshed_on']\n",
    "\n",
    "print(f'The ICTRP shows {len(df_cond)} trials as of {this_extract_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POINT THIS TO LAST WEEK'S PROCESSED DATA\n",
    "last_weeks_trials = pd.read_csv('last_weeks_data/trial_list_2020-05-19.csv').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining in the 'first_seen' field\n",
    "df_cond = df_cond.merge(last_weeks_trials[['trialid', 'first_seen']], left_on = 'TrialID', right_on = 'trialid', \n",
    "                        how='left').drop('trialid', axis=1)\n",
    "\n",
    "#Adding the `first_seen` field to new trials\n",
    "df_cond['first_seen'] = pd.to_datetime(df_cond['first_seen'].fillna(this_extract_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClinicalTrials.gov                 1590\n",
       "ChiCTR                              688\n",
       "IRCT                                171\n",
       "EU Clinical Trials Register         161\n",
       "German Clinical Trials Register      69\n",
       "CTRI                                 66\n",
       "ANZCTR                               47\n",
       "Netherlands Trial Register           38\n",
       "ISRCTN                               35\n",
       "JPRN                                 31\n",
       "REBEC                                11\n",
       "TCTR                                  9\n",
       "RPCEC                                 8\n",
       "PACTR                                 6\n",
       "LBCTR                                 2\n",
       "CRIS                                  2\n",
       "REPEC                                 1\n",
       "SLCTR                                 1\n",
       "Name: Source_Register, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for which registries we are dealing with:\n",
    "df_cond.Source_Register.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with data straight from XML we need to do some tedious tidying up of dates because of different formats. They do not parse correctly by default in Pandas. They are standardized, however, in the ICTRP spreadsheet so I have removed this code for now. It is archived in old commits to the GitHub repo for future refrence if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded 13 trials from before 2020\n",
      "2923 trials remain\n"
     ]
    }
   ],
   "source": [
    "#lets get rid of trials from before 2020 for now\n",
    "\n",
    "pre_2020 = len(df_cond[df_cond['Date_registration'] < pd.Timestamp(2020,1,1)])\n",
    "\n",
    "print(f'Excluded {pre_2020} trials from before 2020')\n",
    "\n",
    "df_cond_rec = df_cond[df_cond['Date_registration'] >= pd.Timestamp(2020,1,1)].reset_index(drop=True)\n",
    "\n",
    "print(f'{len(df_cond_rec)} trials remain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code removes trials that indicate that they never started. This is done on the Chinese registry through specific language in the Title. Trials from ClinicalTrials.gov are indicated by the `Withdrawn` trial status. \n",
    "\n",
    "This will be expanded moving forward to account for the unique terminology used by other registries as necessary moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded 44 cancelled trials with no enrollment\n",
      "Excluded 12 non-COVID trials screened through manual review\n",
      "2869 trials remain\n"
     ]
    }
   ],
   "source": [
    "#Removing cancelled/withdrawn trials for what registries we have to date\n",
    "\n",
    "cancelled_trials = len(df_cond_rec[(df_cond_rec['Public_title'].str.contains('Cancelled')) | \n",
    "                                   (df_cond_rec['Public_title'].str.contains('Retracted due to')) | \n",
    "                                   (df_cond_rec['Recruitment_Status'] == \"Withdrawn\")])\n",
    "\n",
    "print(f'Excluded {cancelled_trials} cancelled trials with no enrollment')\n",
    "\n",
    "#Now lets get rid of trials we know don't belong from manual review, reason catalogued below\n",
    "#NCT04226157 Non-COVID trial delayed because of COVID and put this in the title\n",
    "#NCT04246242 This trial registration doesn't exist anymore\n",
    "#NCT04337320 is monitoring of complications from a device in the context of COVID but has nothing\n",
    "#to do with COVID monitoring or treatment.\n",
    "#NCT03680274 simply allows inclusion of COVID pts but has nothing to do with COVID\n",
    "#JPRN-UMIN000040188 is a systematic review, not an individual study\n",
    "#NCT04278404 is not really a study of COVID-19. Patients with COVID-19 can be included.\n",
    "#NCT04372069 This trial registration doesn't exist anymore\n",
    "#NCT04331860 This trial registration doesn't exist anymore\n",
    "#NCT04337216 This trial registration doesn't exist anymore\n",
    "#NCT04343677 This trial registration doesn't exist anymore\n",
    "#EUCTR2020-001370-30-DE is a duplicate\n",
    "#NCT04386980 is not a COVID-19 study\n",
    "\n",
    "exclude = ['NCT04226157', 'NCT04246242', 'NCT04337320', 'NCT03680274', 'JPRN-UMIN000040188', 'NCT04278404', \n",
    "           'NCT04372069', 'NCT04331860', 'NCT04337216', 'NCT04343677', 'EUCTR2020-001370-30-DE', 'NCT04386980']\n",
    "\n",
    "print(f'Excluded {len(exclude)} non-COVID trials screened through manual review')\n",
    "\n",
    "df_cond_nc = df_cond_rec[~((df_cond_rec['Public_title'].str.contains('Cancelled')) | \n",
    "                           (df_cond_rec['Public_title'].str.contains('Retracted due to'))) & \n",
    "                         ~(df_cond_rec['Recruitment_Status'] == \"Withdrawn\") &\n",
    "                         ~(df_cond_rec['TrialID'].isin(exclude))].reset_index(drop=True)\n",
    "\n",
    "print(f'{len(df_cond_nc)} trials remain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general rule, simply for quality and ease of use, we will usually default to a ClinicalTrials.gov record over another type of registration in instances of cross-registration. The ICTRP alerts users to trials with known cross-registrations in the \"Bridge\" field of their dataset and only lists 1 registration per trial (but does not tell you the cross-registered trial IDs). We can manually check and catalogue these. However we will want to replace some of these when the \"Parent\" registry is another registry. The first step is to remove the duplicate or replaced entries, then we will add the ClinicalTrials.gov (or another) version of the registry entry  back into the dataset when we append known trials. We will then join in a new column listing the known cross-registered trial ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 known cross registrations will be replaced\n"
     ]
    }
   ],
   "source": [
    "c_reg = pd.read_excel('manual_data.xlsx', sheet_name = 'cross registrations')\n",
    "replace_ids = c_reg.id_to_replace.tolist()\n",
    "\n",
    "replaced = df_cond_nc[df_cond_nc.TrialID.isin(replace_ids)]\n",
    "print(f'{len(replaced)} known cross registrations will be replaced')\n",
    "\n",
    "df_cond_nc = df_cond_nc[~(df_cond_nc.TrialID.isin(replace_ids))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for checking changes\n",
    "\n",
    "def trial_diffs(new=True):\n",
    "    df = df_cond_nc.merge(last_weeks_trials['trialid'], left_on = 'TrialID', right_on = 'trialid', how='outer', indicator=True)\n",
    "    if new:\n",
    "        new = df[(df['_merge'] == 'left_only')]\n",
    "        return new['TrialID'].tolist()\n",
    "    else:\n",
    "        dropped = df[(df['_merge'] == 'right_only')]\n",
    "        return dropped['trialid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are approximately 199 new trials\n",
      "The following trials were removed since the last time and were manually checked to confirm:\n",
      "['NCT04324606']\n"
     ]
    }
   ],
   "source": [
    "additions = pd.read_excel('manual_data.xlsx', sheet_name = 'additional_trials').drop('from', \n",
    "                                                                                     axis=1).reset_index(drop=True)\n",
    "\n",
    "print(f'There are approximately {len(trial_diffs(new=True))} new trials')\n",
    "\n",
    "added = additions.TrialID.tolist()\n",
    "\n",
    "print(f'The following trials were removed since the last time and were manually checked to confirm:')\n",
    "print(list(set(trial_diffs(new=False)) - set(added) - set(replace_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to take a quick look at trials that came and went since the last update. We can add in any additional trials that we know about that are not accounted for in the ICTRP database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we check to see if any of our manual additions have been added to the dataset\n",
    "\n",
    "for t in added:\n",
    "    if t in df_cond_nc.TrialID.tolist():\n",
    "        print(f'{t} is already in the data')\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An additional 7 known trials, or preferred cross registrations were added to the data\n",
      "['NCT04323527', 'NCT04335552', 'NCT04336748', 'NCT04341727', 'RBR-9ktwx6', 'NCT04357444', 'NCT04360707']\n",
      "The final dataset is 2840 trials\n"
     ]
    }
   ],
   "source": [
    "#These are trials we know about that are not showing up in the ICTRP data pull \n",
    "#or are re-added as cross-registrations\n",
    "\n",
    "print(f'An additional {len(additions)} known trials, or preferred cross registrations were added to the data')\n",
    "\n",
    "print(added)\n",
    "\n",
    "df_cond_all = df_cond_nc.append(additions)\n",
    "df_cond_all['Date_enrollement'] = df_cond_all['Date_enrollement'].apply(fix_dates)\n",
    "\n",
    "print(f'The final dataset is {len(df_cond_all)} trials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This ensures our check for retrospective registration is accurate w/r/t cross-registrations\n",
    "\n",
    "c_r_comp_dates = c_reg[['trial_id_keep', 'cross_reg_date']].groupby('trial_id_keep', as_index=False).min()\n",
    "c_r_merged = c_r_comp_dates.merge(df_cond_nc[['TrialID', 'Date_registration', 'Date_enrollement']], \n",
    "                                 left_on='trial_id_keep', right_on='TrialID', how='left')\n",
    "c_r_merged['earliest_reg'] = c_r_merged[['cross_reg_date', 'Date_registration']].min(axis=1)\n",
    "pre_reg = c_r_merged[c_r_merged.TrialID.notnull() & (c_r_merged.earliest_reg <= c_r_merged.Date_enrollement)].trial_id_keep.to_list()\n",
    "\n",
    "ret_reg = c_r_merged[c_r_merged.TrialID.notnull() & ~(c_r_merged.earliest_reg <= c_r_merged.Date_enrollement)].trial_id_keep.to_list()\n",
    "ret_reg\n",
    "\n",
    "for index, row in df_cond_all.iterrows():\n",
    "    if row.TrialID in pre_reg:\n",
    "        df_cond_all.at[index, 'retrospective_registration'] = True\n",
    "    elif row.TrialID in ret_reg:\n",
    "        df_cond_all.at[index, 'retrospective_registration'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally, add cross-registration field\n",
    "\n",
    "df_cond_all = df_cond_all.merge(c_reg[['trial_id_keep', 'additional_ids']].drop_duplicates(), \n",
    "                              left_on='TrialID', \n",
    "                              right_on='trial_id_keep', \n",
    "                              how='left').drop('trial_id_keep', axis=1).rename(columns=\n",
    "                                                                               {'additional_ids':\n",
    "                                                                                'cross_registrations'}\n",
    "                                                                              ).reset_index(drop=True)\n",
    "\n",
    "df_cond_all['cross_registrations'] = df_cond_all['cross_registrations'].fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation and data cleaning of all fields. This will be expanded each update as more trials get added and more registries start to add trials with their own idiosyncratic data categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China\n",
      "No Country Given\n",
      "Hong Kong\n",
      "France\n",
      "Sweden\n",
      "Italy\n",
      "Japan, Australia, Asia, Europe\n",
      "Jordan\n",
      "Singapore\n",
      "Netherlands\n",
      "United Kingdom\n",
      "Turkey\n",
      "Belgium\n",
      "United States\n",
      "Norway\n",
      "Australia, United States, China, Indonesia, South Korea, Italy, Vietnam, Singapore, United Kingdom, Japan, Hong Kong, Thailand\n",
      "Italy, China\n",
      "Romania\n",
      "Iran\n",
      "Denmark\n",
      "Spain\n",
      "Switzerland\n",
      "Australia\n",
      "Israel\n",
      "Germany\n",
      "Pakistan\n",
      "Bolivia\n",
      "Canada\n",
      "Thailand\n",
      "United States, United Kingdom\n",
      "Egypt\n",
      "France, Norway, Denmark, Germany\n",
      "Ireland\n",
      "India\n",
      "Greece\n",
      "New Zealand\n",
      "Colombia\n",
      "Australia, United States, Italy, Germany, Spain, China\n",
      "India, Ghana, Nigeria, South Africa, United Kingdom\n",
      "Japan\n",
      "Brazil\n",
      "Mexico\n",
      "Malaysia\n",
      "Pakistan, Nigeria\n",
      "Indonesia\n",
      "Romania, Germany\n",
      "Austria\n",
      "Poland\n",
      "Russian Federation\n",
      "Uruguay, Brazil, Chile\n",
      "Tunisia\n",
      "Slovenia\n",
      "Puerto Rico\n",
      "Liechtenstein\n",
      "Portugal\n",
      "Timor-Leste, Mauritania, Western Sahara, Afghanistan, Virgin Islands, U.S., Bosnia and Herzegovina, Laos, Puerto Rico, Argentina, United States Minor Outlying Islands, Ethiopia, Micronesia, Federated States of, Niger, Singapore, Svalbard and Jan Mayen, Mayotte, Estonia, Syria, Bonaire Saint Eustatius and Saba, Oman, Kiribati, Malawi, Virgin Islands, British, Mongolia, Sudan, Taiwan, United Kingdom, Curacao, Djibouti, Jamaica, Saint Pierre and Miquelon, Guinea-Bissau, Vanuatu, Benin, China, Norway, Honduras, Netherlands, Antarctica, Kuwait, Costa Rica, Greenland, Spain, Albania, Cocos (Keeling) Islands, Kyrgyzstan, Northern Mariana Islands, Belgium, Marshall Islands, Austria, Uzbekistan, Australia, Tunisia, Georgia, Guinea, Montenegro, Armenia, Bahamas, Pitcairn, Seychelles, Cyprus, Mexico, Guatemala, Grenada, Moldova, Hong Kong, Slovakia, Aruba, Rwanda, French Guiana, Libya, South Sudan, Turkey, Macao, Nepal, Germany, Maldives, Romania, Colombia, Haiti, Lebanon, Iraq, Venezuela, Morocco, Netherlands Antilles, Comoros, Saint Helena, Swaziland, Burkina Faso, Sint Maarten (Dutch part), Bhutan, Cook Islands, France, Latvia, India, United States, Somalia, Sweden, Barbados, Tokelau, Falkland Islands, Zambia, New Zealand, Kosovo, Anguilla, Saint Lucia, Congo, Democratic Republic, Holy See (Vatican City State), Kenya, Iceland, New Caledonia, Samoa, Tonga, French Polynesia, Luxembourg, Paraguay, Italy, Philippines, Uganda, Azerbaijan, Myanmar, Hungary, Ukraine, Yemen, Canada, San Marino, Qatar, Ghana, Iran, Kazakhstan, Ireland, South Africa, Ecuador, Heard Island and Mcdonald Islands, Croatia, Greece, Namibia, French Southern Territories, Sao Tome and Principe, Bermuda, Brunei, Nigeria, Botswana, Nauru, Nicaragua, Faroe Islands, South Georgia and the South Sandwich Is, Poland, Reunion, Sierra Leone, Liberia, Chile, Saudi Arabia, Christmas Island, Dominican Republic, Belize, Lithuania, Indonesia, Guernsey, Slovenia, Burundi, Saint Vincent and the Grenadines, Mali, Suriname, Tajikistan, Guadeloupe, Bulgaria, Thailand, Cambodia, Mauritius, Gabon, Mozambique, Saint Martin (French part), American Samoa, Liechtenstein, Martinique, Saint Barthelemy, Turkmenistan, Wallis and Futuna, Pakistan, Gambia, Gibraltar, Dominica, Denmark, Bangladesh, Saint Kitts and Nevis, Togo, Angola, Panama, Antigua and Barbuda, Zimbabwe, Brazil, Vietnam, Central African Republic, Chad, Peru, Bolivia, Jersey, Guam, Isle of Man, Bahrain, Israel, Belarus, Madagascar, Czech Republic, Korea, South, Lesotho, Jordan, Fiji, Palestinian Territory, Cuba, Congo, Monaco, Switzerland, Malaysia, Malta, Sri Lanka, Algeria, Cape Verde, Equatorial Guinea, Tanzania, Guyana, Serbia, Turks and Caicos Islands, Bouvet Island, Cameroon, Uruguay, British Indian Ocean Territory, Korea, North, Finland, Senegal, United Arab Emirates, Portugal, Tuvalu, Papua New Guinea, Cayman Islands, Egypt, Trinidad and Tobago, El Salvador, Norfolk Island, Andorra, Eritrea, Cote d'Ivoire, Montserrat, Solomon Islands, Macedonia, Japan, Niue, Russian Federation, Palau\n",
      "United States, Spain, Canada\n",
      "Brazil, Turkey, Germany, Ireland, South Africa, Lebanon, Croatia, Greece, French Southern Territories, Isle of Man, Israel, Czech Republic, United Kingdom, France, Norway, United States, Netherlands, Lithuania, Sweden, Slovenia, Switzerland, Malta, New Zealand, Spain, Bulgaria, Belgium, Finland, Austria, Australia, Portugal, Luxembourg, Liechtenstein, Italy, Cyprus, Denmark, Slovakia\n",
      "United States, Austria, China\n",
      "Iraq\n",
      "Germany, Hungary, Czech Republic, United Kingdom, France, Denmark\n",
      "Italy, Romania, United Kingdom\n",
      "Belgium, Italy\n",
      "Netherlands, Italy, Germany, Spain, United States, United Kingdom, France, Denmark, Canada\n",
      "Germany, Austria\n",
      "Croatia\n",
      "Argentina\n",
      "Brazil, Qatar, Argentina, Iran, Germany, Ireland, South Africa, Lebanon, Peru, Israel, Norway, Saudi Arabia, Honduras, India, Switzerland, Indonesia, Malaysia, Spain, Kenya, Thailand, Italy, Philippines, Canada\n",
      "Ukraine\n",
      "South Africa\n",
      "United States, Canada\n",
      "Serbia\n",
      "France, Luxembourg\n",
      "Germany, Greece, Israel, United Kingdom, France, Czech Republic, Poland, Netherlands, Lithuania, Sweden, Switzerland, Spain, Belgium, Austria, Portugal, Italy, Hungary, Denmark, Russian Federation\n",
      "Australia, Portugal, Switzerland, Israel, Malaysia, Italy, Germany, Romania, Gambia, Canada, Singapore, France, Poland, China\n",
      "Lebanon\n",
      "Brazil, Qatar, Bosnia and Herzegovina, Argentina, Colombia, Germany, Ireland, Romania, South Africa, Ecuador, Lebanon, Peru, Greece, Estonia, Israel, Nigeria, Czech Republic, United Kingdom, Latvia, Chile, India, United States, Netherlands, Indonesia, Malta, Spain, Belgium, Austria, Thailand, Australia, Portugal, Tunisia, United Arab Emirates, Egypt, Italy, Uganda, Mexico, Guatemala, Hungary, Japan, Russian Federation, Canada\n",
      "Australia, India, Israel, Malaysia, Italy, Germany, Spain, United States, United Kingdom, France, Belgium, Austria, Canada\n",
      "Finland\n",
      "Italy, Germany, Spain, United States, United Kingdom, France\n",
      "Vietnam\n",
      "Italy, Spain, United States, United Kingdom, France, Belgium\n",
      "Hungary\n",
      "Germany, Spain, Mexico, United States, Denmark, Singapore, United Kingdom, Japan, Greece, South Korea\n",
      "Belgium, Italy, United Kingdom\n",
      "Ecuador\n",
      "United States, United States Minor Outlying Islands\n",
      "Israel, Spain, United States, United Kingdom, France, Austria\n",
      "Bosnia and Herzegovina\n",
      "Gibraltar\n",
      "Denmark, Germany, Austria\n",
      "Australia, Norway, Sweden, Israel, Italy, Germany, Spain, Albania, United States, Lebanon, Taiwan, Poland, France, Czech Republic, Hong Kong\n",
      "Australia, Zambia, Ireland, South Africa, United States, United Kingdom, Canada\n",
      "Saudi Arabia\n",
      "Iceland\n",
      "United Kingdom, Ireland\n",
      "Australia, United States, Germany\n",
      "Chile\n",
      "Luxembourg\n",
      "Brazil, Argentina, Italy, Germany, Spain, Singapore, France, Austria, Chile\n",
      "United States, Germany, Sweden\n",
      "Germany, Spain, Mexico, United States, United Kingdom, France\n",
      "Portugal, Switzerland, Italy, Germany, Spain, Ireland, United Kingdom, France, Belgium, Iceland, Austria\n",
      "South Korea\n",
      "Nigeria\n",
      "Japan, Asia(except Japan), North America, South America, Australia, Europe, Africa\n",
      "Cuba\n",
      "Ghana\n",
      "Kenya\n",
      "Portugal, Brazil, Colombia, Spain, United States, United Kingdom, France, Chile, Canada\n",
      "Sri Lanka\n",
      "Bangladesh\n",
      "Italy, India, United States, United Kingdom\n",
      "Malaysia, Italy, United Kingdom, Thailand\n",
      "France, Italy, Spain\n",
      "Belarus\n",
      "Sudan\n",
      "Brazil, Colombia, Spain, United States, France\n",
      "France, Monaco\n",
      "Germany, Sweden\n",
      "France, Germany, Spain\n",
      "France, Belgium, United States, Netherlands\n",
      "Taiwan\n",
      "Iran, United States, Egypt\n",
      "France, French Guiana\n",
      "Iran, Netherlands, Sweden, Switzerland, Italy, Germany, Spain, United States, Singapore, United Kingdom, Taiwan, France, Japan, Hong Kong, South Korea, China\n",
      "Germany, Romania, Ireland, Greece, Estonia, Israel, United Kingdom, France, Czech Republic, Poland, Netherlands, Switzerland, Slovenia, Spain, United States, Belgium, Iceland, Austria, Australia, Portugal, Italy, Cyprus, Hungary, Slovakia, Canada\n",
      "Netherlands, Sweden, Switzerland, Italy, Germany, Spain, United States, Singapore, United Kingdom, Taiwan, France, Japan, Hong Kong, South Korea, China\n",
      "French Guiana\n",
      "Israel, United States\n",
      "Israel, Italy, Germany, Spain, France, Japan, Russian Federation, Canada\n",
      "Turkey, Argentina, Puerto Rico, Germany, Lebanon, Venezuela, Bolivia, Nigeria, Czech Republic, Chile, Saudi Arabia, Netherlands, Costa Rica, Spain, United States, Australia, Tunisia, Italy, Mexico, Guatemala\n",
      "Bahrain\n",
      "United States, Germany, Spain, United Kingdom\n",
      "United States, Puerto Rico\n",
      "Sweden, Italy, Germany, United Kingdom, France, Finland\n",
      "Argentina, United States, Denmark, United Kingdom\n",
      "Egypt, Germany, United States, Singapore, Peru\n",
      "Italy, Germany, Spain, United Kingdom, France, Denmark\n",
      "United States, Poland\n",
      "France, Italy\n",
      "Senegal\n",
      "North Macedonia\n",
      "San Marino, Italy\n",
      "Oman, Saudi Arabia, United Arab Emirates\n",
      "France, United States, United Kingdom, Spain\n",
      "Australia, New Zealand\n",
      "France, Belgium, United States\n",
      "Australia, Portugal, Norway, India, Italy, Spain, Ireland, France, Canada\n"
     ]
    }
   ],
   "source": [
    "#A small function to help quickly check the contents of various columns\n",
    "\n",
    "def check_fields(field):\n",
    "    return df_cond_all[field].unique()\n",
    "\n",
    "#check_fields('Phase')\n",
    "\n",
    "#Check fields for new unique values that require normalisation\n",
    "for x in check_fields('Countries'):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning various fields. \n",
    "#One important thing we have to do is make sure there are no nulls or else the data won't properly load onto the website\n",
    "\n",
    "#semi-colons in the intervention field mess with CSV\n",
    "df_cond_all['Intervention'] = df_cond_all['Intervention'].str.replace(';', '')\n",
    "\n",
    "#Study Type\n",
    "obv_replace = ['Observational [Patient Registry]', 'observational']\n",
    "int_replace = ['interventional', 'Interventional clinical trial of medicinal product', 'Treatment', \n",
    "               'INTERVENTIONAL', 'Intervention', 'Interventional Study']\n",
    "hs_replace = ['Health services reaserch', 'Health Services reaserch', 'Health Services Research']\n",
    "\n",
    "df_cond_all['Study_type'] = (df_cond_all['Study_type'].str.replace(' study', '')\n",
    "                             .replace(obv_replace, 'Observational').replace(int_replace, 'Interventional')\n",
    "                             .replace('Epidemilogical research', 'Epidemiological research')\n",
    "                             .replace(hs_replace, 'Health services research')\n",
    "                             .replace('Others,meta-analysis etc', 'Other'))\n",
    "\n",
    "#phase\n",
    "df_cond_all['Phase'] = df_cond_all['Phase'].fillna('Not Applicable')\n",
    "na = ['0', 'Retrospective study', 'Not applicable', 'New Treatment Measure Clinical Study', 'Not selected', \n",
    "      'Phase 0', 'Diagnostic New Technique Clincal Study', '0 (exploratory trials)']\n",
    "p1 = ['1', 'Early Phase 1', 'I', 'Phase-1']\n",
    "p12 = ['1-2', '2020-02-01 00:00:00', 'Phase I/II', 'Phase 1 / Phase 2', 'Phase 1/ Phase 2',\n",
    "       'Human pharmacology (Phase I): yes\\nTherapeutic exploratory (Phase II): yes\\nTherapeutic confirmatory - (Phase III): no\\nTherapeutic use (Phase IV): no\\n']\n",
    "p2 = ['2', 'II', 'Phase II', 'IIb', 'Phase-2', 'Phase2',\n",
    "      'Human pharmacology (Phase I): no\\nTherapeutic exploratory (Phase II): yes\\nTherapeutic confirmatory - (Phase III): no\\nTherapeutic use (Phase IV): no\\n']\n",
    "p23 = ['Phase II/III', '2020-03-02 00:00:00', 'II-III', 'Phase 2 / Phase 3', 'Phase 2/ Phase 3',\n",
    "       'Human pharmacology (Phase I): no\\nTherapeutic exploratory (Phase II): yes\\nTherapeutic confirmatory - (Phase III): yes\\nTherapeutic use (Phase IV): no\\n']\n",
    "p3 = ['3', 'Phase III', 'Phase-3', \n",
    "      'Human pharmacology (Phase I): no\\nTherapeutic exploratory (Phase II): no\\nTherapeutic confirmatory - (Phase III): yes\\nTherapeutic use (Phase IV): no\\n']\n",
    "p34 = ['Phase 3/ Phase 4', \n",
    "       'Human pharmacology (Phase I): no\\nTherapeutic exploratory (Phase II): no\\nTherapeutic confirmatory - (Phase III): yes\\nTherapeutic use (Phase IV): yes\\n']\n",
    "p4 = ['4', 'IV', \n",
    "      'Human pharmacology (Phase I): no\\nTherapeutic exploratory (Phase II): no\\nTherapeutic confirmatory - (Phase III): no\\nTherapeutic use (Phase IV): yes\\n']\n",
    "\n",
    "df_cond_all['Phase'] = (df_cond_all['Phase'].replace(na, 'Not Applicable').replace(p1, 'Phase 1')\n",
    "                        .replace(p12, 'Phase 1/Phase 2').replace(p2, 'Phase 2')\n",
    "                        .replace(p23, 'Phase 2/Phase 3').replace(p3, 'Phase 3').replace(p34, 'Phase 3/Phase 4')\n",
    "                        .replace(p4, 'Phase 4'))\n",
    "\n",
    "#Fixing Observational studies incorrectly given a Phase in ICTRP data\n",
    "m = ((df_cond_all.Phase.str.contains('Phase')) & (df_cond_all.Study_type == 'Observational'))\n",
    "df_cond_all['Phase'] = df_cond_all.Phase.where(~m, 'Not Applicable')\n",
    "\n",
    "#Recruitment Status\n",
    "df_cond_all['Recruitment_Status'] = df_cond_all['Recruitment_Status'].replace('Not recruiting', 'Not Recruiting')\n",
    "df_cond_all['Recruitment_Status'] = df_cond_all['Recruitment_Status'].fillna('No Status Given')\n",
    "\n",
    "#Get rid of messy accents\n",
    "def norm_names(x):\n",
    "    normed = unicodedata.normalize('NFKD', str(x)).encode('ASCII', 'ignore').decode()\n",
    "    return normed \n",
    "\n",
    "df_cond_all['Primary_sponsor'] = df_cond_all.Primary_sponsor.apply(norm_names)\n",
    "df_cond_all['Primary_sponsor'] = df_cond_all['Primary_sponsor'].replace('NA', 'No Sponsor Name Given')\n",
    "df_cond_all['Primary_sponsor'] = df_cond_all['Primary_sponsor'].replace('nan', 'No Sponsor Name Given')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Countries\n",
    "df_cond_all['Countries'] = df_cond_all['Countries'].fillna('No Country Given').replace('??', 'No Country Given')\n",
    "\n",
    "china_corr = ['Chian', 'China?', 'Chinese', 'Wuhan', 'Chinaese', 'china', 'Taiwan, Province Of China', \n",
    "              \"The People's Republic of China\"]\n",
    "\n",
    "country_values = df_cond_all['Countries'].tolist()\n",
    "\n",
    "new_list = []\n",
    "\n",
    "for c in country_values:\n",
    "    country_list = []\n",
    "    if isinstance(c, float):\n",
    "        country_list.append('No Sponsor Name Given')\n",
    "    elif c == 'No Sponsor Name Given':\n",
    "        country_list.append('No Sponsor Name Given')\n",
    "    elif c in china_corr:\n",
    "        country_list.append('China')\n",
    "    elif c in ['Iran (Islamic Republic of)', 'Iran, Islamic Republic of']:\n",
    "        country_list.append('Iran')\n",
    "    elif c in ['Viet nam', 'Viet Nam']:\n",
    "        country_list.append('Vietnam')\n",
    "    elif c in ['Korea, Republic of', 'Korea, Republic Of', 'KOREA'] :\n",
    "        country_list.append('South Korea')\n",
    "    elif c in ['USA', 'United States of America', 'U.S.']:\n",
    "        country_list.append('United States')\n",
    "    elif c == 'Japan,Asia(except Japan),Australia,Europe':\n",
    "        country_list = ['Japan', 'Australia', 'Asia', 'Europe']\n",
    "    elif c == 'Japan,Asia(except Japan),North America,South America,Australia,Europe,Africa':\n",
    "        country_list = ['Japan, Asia(except Japan), North America, South America, Australia, Europe, Africa']\n",
    "    elif c == 'The Netherlands':\n",
    "        country_list.append('Netherlands')\n",
    "    elif c == 'England':\n",
    "        country_list.append('United Kingdom')\n",
    "    elif c == 'Japan,North America':\n",
    "        country_list = ['Japan', 'North America']\n",
    "    elif c == 'Czechia':\n",
    "        country_list.append('Czech Republic')\n",
    "    elif ';' in c:\n",
    "        c_list = c.split(';')\n",
    "        unique_values = list(set(c_list))\n",
    "        for v in unique_values:\n",
    "            if v in china_corr:\n",
    "                country_list.append('China')\n",
    "            elif v in ['Iran (Islamic Republic of)', 'Iran, Islamic Republic of']:\n",
    "                country_list.append('Iran')\n",
    "            elif v in ['Korea, Republic of', 'Korea, Republic Of', 'KOREA']:\n",
    "                country_list.append('South Korea')\n",
    "            elif v in ['Viet nam', 'Viet Nam']:\n",
    "                country_list.append('Vietnam')\n",
    "            elif v in ['USA', 'United States of America']:\n",
    "                country_list.append('United States')\n",
    "            elif v == 'The Netherlands':\n",
    "                country_list.append('Netherlands')\n",
    "            elif v == 'England':\n",
    "                country_list.append('United Kingdom')\n",
    "            elif v == 'Czechia':\n",
    "                country_list.append('Czech Republic')\n",
    "            else:\n",
    "                country_list.append(v)\n",
    "    else:\n",
    "        country_list.append(c.strip())\n",
    "    new_list.append(', '.join(country_list))\n",
    "\n",
    "df_cond_all['Countries'] = new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last space for manual intervention. This will include manual normalisation of new names, any updates to the normalisation schedule from the last update, and updating manually-coded intervention type data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update the normalisation schedule and rerun\n"
     ]
    }
   ],
   "source": [
    "#Normalizing sponsor names\n",
    "#Run this cell, updating the spon_norm csv you are loading after manual adjusting\n",
    "#until you get the 'All sponsor names normalized' to print\n",
    "\n",
    "spon_norm = pd.read_excel('manual_data.xlsx', sheet_name = 'sponsor')\n",
    "\n",
    "df_cond_norm = df_cond_all.merge(spon_norm, left_on = 'Primary_sponsor', right_on ='unique_spon_names', how='left')\n",
    "df_cond_norm = df_cond_norm.drop('unique_spon_names', axis=1)\n",
    "\n",
    "new_unique_spon_names = (df_cond_norm[df_cond_norm['normed_spon_names'].isna()][['Primary_sponsor', 'TrialID']]\n",
    "                        .groupby('Primary_sponsor').count())\n",
    "\n",
    "if len(new_unique_spon_names) > 0:\n",
    "    new_unique_spon_names.to_csv('to_norm.csv')\n",
    "    print('Update the normalisation schedule and rerun')\n",
    "else:\n",
    "    print('All sponsor names normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update the intervention type assessments and rerun\n"
     ]
    }
   ],
   "source": [
    "#Integrating intervention type data\n",
    "#Once again, run to bring in the old int-type data, islolate the new ones, update, and rerun until\n",
    "#producing the all-clear message\n",
    "\n",
    "int_type = pd.read_excel('manual_data.xlsx', sheet_name = 'intervention')\n",
    "df_cond_int = df_cond_norm.merge(int_type[['trial_id', 'study_category',\n",
    "                                           'intervention', 'intervention_list']], \n",
    "                                 left_on = 'TrialID', right_on = 'trial_id', how='left')\n",
    "\n",
    "df_cond_int = df_cond_int.drop('trial_id', axis=1)\n",
    "\n",
    "new_int_trials = df_cond_int[(df_cond_int['study_category'].isna()) | (df_cond_int['intervention'].isna())]\n",
    "\n",
    "if len(new_int_trials) > 0:\n",
    "    new_int_trials[['TrialID', 'Public_title', 'Intervention', 'study_category', \n",
    "                    'intervention', 'intervention_list']].to_csv('int_to_assess.csv')\n",
    "    print('Update the intervention type assessments and rerun')\n",
    "else:\n",
    "    print('All intervention types matched')\n",
    "    df_cond_int = df_cond_int.drop('Intervention', axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can use this cell to output counts of values from columns\n",
    "\n",
    "treatments = df_cond_int[df_cond_int.study_category == 'Drug']['study_category'].tolist()\n",
    "countries = df_cond_int.Countries.to_list()\n",
    "\n",
    "def var_counts(var_list, split_char, lower=False):\n",
    "    final_list = []\n",
    "    for v in var_list:\n",
    "        t_list = v.split(split_char)\n",
    "        for l in t_list:\n",
    "            if lower:\n",
    "                final_list.append(l.lower().strip())\n",
    "            else:\n",
    "                final_list.append(l.strip())\n",
    "    return Counter(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dates = pd.read_excel('manual_data.xlsx', sheet_name = 'Completion Dates')\n",
    "df_comp_dates = df_cond_int.merge(comp_dates, \n",
    "                                  left_on='TrialID', right_on='trialid', \n",
    "                                  how='left', indicator=True).drop('trialid', axis=1)\n",
    "\n",
    "print(df_comp_dates[df_comp_dates['_merge'] == 'left_only'].TrialID.to_list())\n",
    "\n",
    "df_comp_dates = df_comp_dates.drop('_merge', axis=1).reset_index(drop=True)\n",
    "    \n",
    "df_comp_dates['primary_completion_date'] = (pd.to_datetime(df_comp_dates['primary_completion_date'], \n",
    "                                                          errors='coerce', \n",
    "                                                          format='%Y-%m-%d')\n",
    "                                            .fillna('Not Available').apply(fix_dates))\n",
    "\n",
    "df_comp_dates['full_completion_date'] = (pd.to_datetime(df_comp_dates['full_completion_date'], \n",
    "                                                          errors='coerce', \n",
    "                                                          format='%Y-%m-%d')\n",
    "                                            .fillna('Not Available').apply(fix_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for any results on ICTRP\n",
    "ictrp_results = df_comp_dates[(df_comp_dates.has_results.notnull()) | (df_comp_dates.has_results.notnull())]\n",
    "\n",
    "if len(ictrp_results) > 0:\n",
    "    print(f'There are {len(ictrp_results)} results to check')\n",
    "else:\n",
    "    print('There are no results to check')\n",
    "\n",
    "#If results cross-check with results already collected in 'manual_data' excel file and add any new trial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_excel('manual_data.xlsx', sheet_name = 'Results')\n",
    "df_results = df_comp_dates.merge(results, \n",
    "                                 left_on='TrialID', \n",
    "                                 right_on='trialid', \n",
    "                                 how='left').drop('trialid', axis=1)\n",
    "\n",
    "df_results['results_link'] = df_results['results_link'].fillna('No Results')\n",
    "df_results['results_type'] = df_results['results_type'].fillna('No Results')\n",
    "\n",
    "df_results['results_publication_date'] = (pd.to_datetime(df_results['results_publication_date'], \n",
    "                                                          errors='coerce', \n",
    "                                                          format='%Y-%m-%d')\n",
    "                                            .fillna('No Results').apply(fix_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final organising\n",
    "\n",
    "col_names = []\n",
    "\n",
    "for col in list(df_results.columns):\n",
    "    col_names.append(col.lower())\n",
    "    \n",
    "df_results.columns = col_names\n",
    "\n",
    "reorder = ['trialid', 'source_register', 'date_registration', 'date_enrollement', 'retrospective_registration', \n",
    "           'normed_spon_names', 'recruitment_status', 'phase', 'study_type', 'countries', 'public_title', \n",
    "           'study_category', 'intervention', 'intervention_list', 'target_enrollment', 'primary_completion_date', \n",
    "           'full_completion_date', 'web_address', 'results_type', 'results_publication_date', 'results_link', \n",
    "           'last_refreshed_on', 'first_seen', 'cross_registrations']\n",
    "\n",
    "df_final = df_results[reorder].reset_index(drop=True).drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for any null values\n",
    "df_final[df_final.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick look at the data\n",
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export final dataset\n",
    "df_final.to_csv(f'processed_data_sets/trial_list_{this_extract_date}.csv', index=False)\n",
    "\n",
    "df_final.to_csv(f'tableau_data/current_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_results = df_final[df_final.results_type != 'No Results']\n",
    "\n",
    "results_total = len(just_results)\n",
    "\n",
    "print(f'There are {results_total} trials with results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#Export json for website\n",
    "import json\n",
    "with open(\"website_data/trials_latest.json\", \"w\") as f:\n",
    "    json.dump({\"data\": df_final.astype(str).values.tolist()}, f, indent=2)\n",
    "\n",
    "with open(\"website_data/results_latest.json\", \"w\") as f:\n",
    "    json.dump({\"data\": just_results.astype(str).values.tolist()}, f, indent=2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Trend in Registered Trials Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "just_reg = df_final[['trialid', 'date_registration']].reset_index(drop=True)\n",
    "#just_reg = mar18[['trialid', 'date_registration']].reset_index(drop=True)\n",
    "#just_reg['date_registration'] = pd.to_datetime(just_reg['date_registration'], format='%d/%m/%Y')\n",
    "\n",
    "#catch old registrations that were expanded to include COVID, we can get rid of these for now\n",
    "just_reg = just_reg[just_reg['date_registration'] >= pd.Timestamp(2020,1,1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_reg.index = just_reg['date_registration']\n",
    "\n",
    "\n",
    "grouped = just_reg.resample('W').count()\n",
    "cumsum = grouped.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = []\n",
    "\n",
    "for x in list(grouped.index):\n",
    "    labels.append(str(x.date()))\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(labels)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5), dpi = 300)\n",
    "\n",
    "l1 = plt.plot(x_pos, grouped['trialid'], marker = 'o')\n",
    "l2 = plt.plot(x_pos, cumsum['trialid'], marker = 'o')\n",
    "\n",
    "for i, j in zip(x_pos[1:], grouped['trialid'].tolist()[1:]):\n",
    "    ax.annotate(str(j), (i,j), xytext = (i-.1, j-50))\n",
    "\n",
    "for i, j in zip(x_pos, cumsum['trialid']):\n",
    "    ax.annotate(str(j), (i,j), xytext = (i-.2, j+25))\n",
    "    \n",
    "\n",
    "gr = grouped['trialid'].to_list()\n",
    "cs = cumsum['trialid'].to_list()\n",
    "\n",
    "plt.xticks(x_pos, labels, rotation=45, fontsize=8)\n",
    "plt.ylim(-50, 2500)\n",
    "plt.xlabel('Week Ending Date')\n",
    "plt.ylabel('Registered Trials')\n",
    "plt.title('Registered COVID-19 Trials by Week on the ICTRP')\n",
    "plt.legend(('New Trials', 'Cumulative Trials'), loc=2)\n",
    "#plt.savefig(f'trial_count_{last_extract_date}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "labels = []\n",
    "\n",
    "for x in list(grouped.index):\n",
    "    labels.append(str(x.date()))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=labels[:-1], y=grouped['trialid'][:-1], fill=None, name='New Trials'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=labels[:-1], y=cumsum['trialid'][:-1], fill=None, name='Cumulative Trials'))\n",
    "\n",
    "fig.update_layout(title={'text': 'Registered COVID-19 Trials by Week', 'xanchor': 'center', 'x': 0.5}, \n",
    "                  xaxis_title='Week Ending Date',\n",
    "                  yaxis_title='Registered Trials',\n",
    "                  legend = dict(x=0, y=1, traceorder='normal', bgcolor='rgba(0,0,0,0)'))\n",
    "\n",
    "\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"html_figures/registered trials.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_types = df_final.study_category.value_counts()\n",
    "int_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "treatment_dict = dict(drugs = int_types['Drug'] + int_types['Drug (Chemoprophylaxis)'], \n",
    "                      atmp = int_types['ATMP'], \n",
    "                      clinical_char = (int_types['Clinical Presentation'] + int_types['Diagnostics'] + \n",
    "                                       int_types['Prognosis'] + int_types['Clinical Presentation (Epidemiology)']),\n",
    "                      drug_other_combo = (int_types['Drug (+ ATMP + Other (renal))'] + int_types['Drug (+ ATMP)'] + \n",
    "                                          int_types['ATMP (+ Drug)'] + int_types['Drug (+ chemoprophylaxis)']),\n",
    "                      supp = int_types['Supplement'],\n",
    "                      geno = int_types['Genomics'],\n",
    "                      th = int_types['Telehealth'],\n",
    "                      pro = int_types['Procedure'],\n",
    "                      tm = int_types[[s.startswith('Traditional Medicine') for s in int_types.index]].sum(),\n",
    "                      other = (int_types[[s.startswith('Other') for s in int_types.index]].sum() \n",
    "                               + int_types['Health System'])\n",
    "                     )\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "            x=list(treatment_dict.values()),\n",
    "            y=['Drugs', 'ATMP', 'Clinical Characteristics', 'Multiple Therapies', 'Supplement', 'Genomics', \n",
    "               'Telehealth', 'Procedure', 'Traditional Medicine', 'Other'],\n",
    "            orientation='h'))\n",
    "\n",
    "fig.update_layout(title={'text': 'Intervention Type of Registered Trials', 'xanchor': 'center', 'x': 0.5}, \n",
    "                  xaxis_title='Number of Trials')\n",
    "\n",
    "fig.show()\n",
    "fig.write_html('html_figures/int_bar.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Bar(\n",
    "            x=df_final.source_register.value_counts().values,\n",
    "            y=df_final.source_register.value_counts().index,\n",
    "            orientation='h'))\n",
    "\n",
    "fig.update_layout(title={'text': 'Registered Studies by Trial Registry', 'xanchor': 'center', 'x': 0.55}, \n",
    "                  xaxis_title='Number of Studies Registered',\n",
    "                  yaxis=dict(autorange=\"reversed\"))\n",
    "\n",
    "fig.show()\n",
    "fig.write_html('html_figures/registries_bar.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = df_final[((df_final.study_category.str.contains('Drug')) | (df_final.study_category.str.contains('ATMP'))) & ~(df_final.study_category.str.contains('Traditional Medicine'))]['intervention_list'].tolist()\n",
    "common_treatments = pd.DataFrame(var_counts(treatments, ';', lower=True).most_common())\n",
    "common_treatments.columns = ['treatment', 'trial_count']\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "            x=common_treatments[common_treatments.trial_count >= 15]['trial_count'],\n",
    "            y=common_treatments[common_treatments.trial_count >= 15]['treatment'],\n",
    "            orientation='h'))\n",
    "\n",
    "fig.update_layout(title={'text': 'Most Commonly Studied Drugs & ATMPs (n>=15)', 'xanchor': 'center', 'x': 0.5}, \n",
    "                  xaxis_title='Number of Studies Registered',\n",
    "                  yaxis=dict(autorange=\"reversed\", dtick=1))\n",
    "\n",
    "fig.show()\n",
    "fig.write_html('html_figures/treatment_bar.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "countries = df_final.countries.to_list()\n",
    "most_studies = pd.DataFrame(var_counts(countries, ',', lower=False).most_common())\n",
    "most_studies.columns = ['country', 'trial_count']\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "            x=most_studies[(most_studies.trial_count >= 50) & (most_studies.country != 'No Country Given')]['trial_count'],\n",
    "            y=most_studies[(most_studies.trial_count >= 50) & (most_studies.country != 'No Country Given')]['country'],\n",
    "            orientation='h'))\n",
    "\n",
    "fig.update_layout(title={'text': 'Most Common Study Locations (n>=50)', 'xanchor': 'center', 'x': 0.5}, \n",
    "                  xaxis_title='Number of Studies Registered',\n",
    "                  yaxis=dict(autorange=\"reversed\", dtick=1))\n",
    "\n",
    "fig.show()\n",
    "fig.write_html('html_figures/location_bar.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "notebook_metadata_filter": "all,-language_info",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.3.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
